# Chapter 1: AI Early History and Applications

The notion of gaining knowledge often constitutes a transgression against the laws of god (nature more aptly put) is deeply ingrained into western thought. This can be seen through examples such as the garden of Eden or Prometheus. In other words the quest for knowledge is a dangerous one. 

Some other examples of this phenomenon within literature being Frankenstein monster, as well as Dante and Milton's work, and many Shakespearean plays. Some more practical examples within real life may include the creation of nuclear weapons and now the rising threat of Artificial Intelligence. Modern technology has only caused these fears to rise, creating a sentiment of imminent danger in many.

>[!NOTE]
>In my eyes this seems to follow a narrative of blissful ignorance. To know less is to be less aware of the negatives, thus your view of life is purely subjective and it is possible to alienate yourself from the reality of consequence. 



### Aristotle 
The logical starting point within history for artificial intelligence is Aristotle. **Change** to Aristotle was the basis of science, science being described by him as *"The study of things that change"*.  He distinguished between **matter** and **form**. This separation is the philosophical basis for symbolic computing as well as data abstraction. 

Within Aristotle epistemology (analysis of how humans "know" their world), he refereed to logic as an instrument. This was because he felt the study of thought itself was the basis of all knowledge. Within his Logic he investigated whether propositions can be true because they are related to other statements that are true. 

<blockquote>if we know that “all men are mortal” and that “Socrates
is a man”, then we can conclude that “Socrates is mortal”. </blockquote>



### A Shift In Perspective
Renaissance thought introduced science as a means of replacing mysticism, in order to further understand nature. Scientists and Philosophers alike realized that thought, the way knowledge is represented and manipulated in the human mind, was an essential part of scientific study. The most major development within the modern view of our world, shaping this change, was the Copernican revolution. Due to religion viewing the earth as the center of the solar system, the solar system being created for man, there was a major separation between the scientific observations and the teachings of religion. 

Francis Bacon offered an additional method of scientific methodology being the **features**. Bacon described the form as equivalent to the sum of features. Bacon additionally stated that objects can be described by what features it *does not* possess. This is foundational for the creation of the **Truth Table**.

The Pascaline, create by Blaise Pascal, was a famous calculating machine allowing for arithmetic computation using mechanics. However it was limited to addition and subtraction, but it served as the basis for the automation of mathematical computation. This thought process birthed the idea of automating the actions of humans. This can be seen through Pascals statements in *Pensees* (1670).


<blockquote>The arithmetical machine produces effects which approach
nearer to thought than all the actions of animals"</blockquote> 


Pascals work inspired Hottfried Wilhem von Leibniz in 1694 to produce the Leibniz Wheel. This machine introduced both multiplication and division, marking a major step towards the creation of "Artificial Intelligence". 

**Rene Descartes** was a central figure towards the development of modern thought and our understanding of the mind. Descartes attempted to formulate a basis of reality purely through introspection. This was done  through the systematic rejection of senses since they proved untrustworthy. This lead him to even doubt the physical world, and even his own existence. 


<blockquote>I think therefore I am.</blockquote> 


>[!NOTE]
>I would like to note, here's the philosopher in me coming out, that the statement I think therefore I am presumes the existence of the self circularly. 


This is an important moment within thought, due to it being one of the first separations between the idea of the mind and the physical world. This idea underlies the methodology of AI, as well as many other fields even extending to high level mathematics. 

The foundation for the study of AI, in relation to the separation between the mind and the body, follows that the mind and body are not fundamentally different entities and mental processes can be expressed in terms of mathematics much the same as physical processes. Mental processes are achieved through physical systems (EX. The Brain), therefore they are bound together. 



### AI and Rationalist and Empiricist Traditions

For rationalists the external world is created through the clear and distinct ideas of mathematics. Many AI programs favor rationalist thinking as their basis for creation. Some critics see this rationalist bias as a part of failure on AI for solving complex tasks such as understanding human behavior. 


<blockquote>"Nothing enters the mind except through the senses"</blockquote>

Bayes theorem demonstrates through learning the correlations of effects of actions we can determine the probability of their causes. 

Immanuel Kant was strongly influenced by the writings of Hume. He began to attempt to synthesize the Rationality and Empiricist traditions. 


> [!NOTE]
> Hi me again, Kant was definitely influenced by Hume but 20 years down the drain. 


For Kant knowledge contains two collaborating energies, these are an **a priori** component coming from the subjects reason as well as a **a posteriori** component coming from active experience. 


<blockquote>Kant claims, passing images or representations are bound together by the active subject and taken as the diverse appearances of an identity, of an “object”</blockquote> 



### The Development of Formal Logic
Euler introduced the study of representations that can abstractly capture the structure of relationships in the world as well as discrete steps within a computation about these relationships. The creation of graph theory allowed for state space search, which is used largely within artificial intelligence. The nodes of a state space graph represent the possible stages of a problem solution, the arcs being inferences, moves in a game, or other steps within a problems solution. State space graphs provide a tool for measuring the structure and complexity of problems, as well as analyzing the efficiency, correctness, and solution strategies.

Charles Babbage **difference engine** was a machine for computing values of certain polynomial functions and was the forerunner to his **analytical engine**. He included within his analytical engine many modern solutions such as the separation of the memory and the processor. He referred to these as the *store* and the *mill*.


<blockquote>
The importance of Boole’s accomplishment is in the extraordinary power and simplicity of
the system he devised: three operations, “AND” (denoted by ∗ or ∧), “OR” (denoted by + or lor), and “NOT” (denoted by ¬), formed the heart of his logical calculus.</blockquote>


Boole's influence has carried to modern forms of logic, including the design of modern computers. Bools system additionally provided a basis for binary arithmetic. Frege later added the logical "IMPLIES" and "EXISTS" operations, which formalized the mathematical concept of a proof using these concepts. First order predicate calculus offers all of the tools necessary in creating a theory of representation for Artificial Intelligence. 


1. A language for expression
2. A theory for assumptions related to the meaning of expressions
3. A logically sound calculus for inferring new true expressions


Russell's and Whiteheads work is very important to the foundations of artificial intelligence. Their goal was to derive the whole of mathematics through formal operations on a collection of axioms.

**Intelligence** is a form of information processing. Formal logic is an important representational tool for AI research. Tools are essential to understanding information, tools shape the basis for semantic networks and models of semantic meaning. A tool is developed to solve a specific problem, it is then refined to fit other use cases, leading to the development of new tools. 

### The Turing Test 



